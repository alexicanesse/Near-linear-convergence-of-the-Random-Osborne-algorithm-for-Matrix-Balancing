\input{preambule.tex}
\addbibresource{rapport.bib}
\usepackage{tkz-base}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{changepage}
\usepackage[]{tocbibind}
\usepackage[useregional]{datetime2}
\usepackage{adjustbox}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{array}
\usepackage{cleveref}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage[multiple]{footmisc}
\usepackage[minNoteWidth=1cm,
    colorinlistoftodos,
    backgroundcolor=magenta!30!white,
    bordercolor=magenta]{luatodonotes}
\usepackage{amssymb}

\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{s}{>{\hsize=.3\hsize\linewidth=\hsize}C}
\newcolumntype{D}{>{\hsize=.4\hsize\linewidth=\hsize}C} %Double width column

% \setlength{\bibhang}{0pt}

% \defbibenvironment{bibliography}
% {\list{\printtext[labelalphawidth]{\printfield{labelprefix}\printfield{labelalpha}}}% Updated line
% {\setlength{\labelwidth}{\labelalphawidth}% Updated line
% \setlength{\leftmargin}{0pt}%
% \setlength{\labelsep}{\biblabelsep}%
% \setlength{\itemsep}{\bibitemsep}%
% \setlength{\itemsep}{0pt}% Set to 0pt to remove vertical space
% \setlength{\parsep}{\bibparsep}}%
% \renewcommand*{\makelabel}[1]{\hss\hspace{\dimexpr\labelalphawidth+\labelsep}##1}}
% {\endlist}
% {\item}

% Redefine the footnotetext command to remove indentation
\makeatletter
\renewcommand\@makefntext[1]{%
    \noindent\makebox[0em][r]{\@thefnmark\hspace{5pt}}#1}
\makeatother

\newcommand{\reftodo}[1][]{\todo[backgroundcolor=green!30!white, bordercolor=green, #1]{\thesubsubsection{}~Ref} {\color{red} REF}}
\newcommand{\missingtext}[1][]{\todo[inline, caption=\thesubsection~Fill missing text, backgroundcolor=cyan!30!white, bordercolor=cyan]{#1}}
%\usepackage[]{todonotes}

\setlength\parindent{0pt}

\usetikzlibrary{shapes.multipart}

\makeatletter
\newenvironment{fixedfigure}
{\def\@captype{figure}\center}
{\endcenter}
\makeatother



% \usepackage{graphicx,txfonts}

\addto\captionsbritish{
    \renewcommand{\contentsname}%
    {Table of content}%
}

\renewcommand{\today}{\number\day\textsuperscript{\scriptsize\text{th}} \DTMenglishmonthname{\month} \number\year}


\makeatletter
\newcommand\setxveclength[5]{% newmacro, node1, node2
    \pgfpointdiff{#2}{#3}
    \edef#1{\the\pgf@x}
}
\makeatother

\sloppy                  

\pgfplotsset{compat=1.16}

\begin{document}


%\setmathfont{Latin Modern Math}
%\setmathfont[range={\mathscr,\mathbfscr}]{XITS Math}

% \maketitle



\begin{titlepage}
    \begin{center}
        \vspace*{1cm}
            
        \Huge
        \textbf{Random Osborne Algorithm for Matrix Balancing}
            
        \vspace{0.5cm}
        \LARGE
        Optimal transport report
            
        \vspace{2.5cm}
            
        \huge {\sc Alexi Canesse\footnote{\href{mailto:alexi.canesse@ens-lyon.fr}{alexi.canesse@ens-lyon.fr}, ENS de Lyon, France}\footnote{\href{mailto:alexi.canesse@ens-paris-saclay.fr}{alexi.canesse@ens-paris-saclay.fr}, ENS Paris-Saclay, France}}\\\Large
            
        \vfill

        Optimal transport course\\
        Part of the MVA program at ENS Paris-Saclay.
            
        \vspace{2cm}
        \includegraphics[width=0.5\textwidth]{./figures/logo.pdf}
        \vspace{2cm}

        \Large
        Computer Science and Mathematics\\
        École Normale Supérieur Paris-Saclay\\
        École Normale Supérieur de Lyon\\
        Paris, France\\
        \today
            
    \end{center}
\end{titlepage}





\newpage

%\tableofcontents
\makeatletter
\section*{Table of content}
\vspace{.05\textwidth}
\@starttoc{toc}
\thispagestyle{empty} % Remove page number from the ToC page
\clearpage % Start the new page without page number

\setcounter{page}{1} % Reset page counter and set page number to 1

\makeatother
\newpage

\section*{Abstract}
% \vspace*{10pt} 
\begin{adjustwidth}{30pt}{30pt}
     \textit{
        We dived in matrix balancing, a crucial task with diverse applications such as an increased stability in eigen-spaces decomposition, and introduces \textsc{Osborn}'s algorithm along with its variants. The focus is on presenting both the theoretical foundations and practical implementation aspects. A comprehensive guide to achieving an efficient implementation is provided, supported by clean, openly available code.
     }\\

     \textit{
        Theoretical results, fundamental to the article, are presented: near-linear convergence. They are showcased through numerical experiments, demonstrating relative success in illustrating their complexities. Furthermore, a connection is established between \textsc{Osborn}'s and \textsc{Sinkhorn}'s algorithms, leading to a thorough comparative analysis across various types of kernels for matrix balancing. The findings highlight instances where \textsc{Sinkhorn} encounters challenges, contrasting with \textsc{Osborn}'s consistent convergence across variants. Leveraging these insights, a compelling demonstration is presented where \textsc{Osborn} excels while \textsc{Sinkhorn} struggles and fails to converge in a constructed kernel.
     }
\end{adjustwidth}
\vspace*{10pt}

\section{Introduction}

The article we dived into for our project~\cite{altschuler2023near} introduces \textsc{Osborn}'s algorithm, a matrix balancing algorithm widely employed in preconditioning matrices for tasks such as eigen-decomposition. The authors provided a significant contribution by proving near-linear runtime in the sparsity of the input matrix. This achievement does not enhances the algorithm's efficiency but gives us a better understanding of its performances.

\subsection{Presentation of the problem}

The matrix balancing problem arises from the recognition that matrices encountered in practical applications may have varying scales across rows and columns. These imbalances can lead to numerical instability and adversely impact the accuracy of numerical computations such as eigenvalues/eigenvectors decomposition~\cite{chen2000balancing, chen2001preconditioning} or solving linear systems~\cite{chen2001preconditioning}, . Matrix balancing algorithms aim to address these issues by adjusting the scaling of rows and columns, thereby enhancing the overall numerical properties of the matrix.\\

Other problems can be reduced to matrix balancing, which gives direct applications. For instance, in optimal transport with problems where decision variables are probability distributions~\cite{altschuler2022transport}; the approximating Min-Mean-Cycle which can be solved in near-linear runtime in the number of edges~\cite{altschuler2022approximating} using results from matrix balancing and in particular, the main result of the studied article. 

\nota{Let \(c\) and \(r\) respectively be the column-wise sum and the row-wise sum of matrices \textit{ie.}
\[
    c : \left\{\begin{array}{rcl}
        \M_{n,m}(\K) & \to &\K\\
        A & \mapsto &  A^\top \mathbf{1}
    \end{array}\right. \quad \text{and} \quad r: \left\{\begin{array}{rcl}
        \M_{n,m}(\K) & \to &\K\\
        A & \mapsto & A \mathbf{1}.
    \end{array}\right.
\]}

\defn{Let \(A \in \M_n(\R_+)\) be a non negative square matrix, \(\varepsilon \geq 0\) and \(k \in \N^\star\). The matrix \(A\) is \((\varepsilon, k\)\)-balanced if 
\begin{align}\label{norm}
    \dfrac{||c(A) - r(A)||_k}{\sum_{i,j} a_{i,j}} \leq \varepsilon.
\end{align}
Furthermore, if \(A\) is \((0, k)\)-balanced, we say that \(A\) is balanced.}

\defn{The \(\varepsilon, k\)-\textbf{approximate matrix balancing problem} is: given a square non-negative matrix \(K \in \M_n(\R_+)\), \(\varepsilon \geq 0\) and \(k \in \N^\star\), find a vector \(x \in \R^n\) such that \(\mathcal D(e^x)K\mathcal D(e^{-x})\) is \((\varepsilon, k)\)-balanced.}

\subsection{Related work}

\paragraph*{Iterative algorithms} The \txtsc{Osborn} algorithm, introduced by \textsc{Osborn} in \cite{osborne1960pre} and later discussed in~\cite{parlett1971balancing}, is implemented in Scipy as a default matrix balancing method. Other iterative methods have been introduces. Notably, the \textsc{Sinkhorn-Knopp} algorithm~\cite{sinkhorn1967concerning}, a special case of \textsc{Bregman}’s balancing method~\cite{lamond1981bregman}, iteratively rescales each row and column until convergence. However, it converges linearly, which is impractical for large and sparse matrices, as discussed by \textsc{Soules} et al. in~\cite{soules1991rate}. \textsc{Parlett} introduced an approach in~\cite{parlett1971balancing} to approximately balance matrices by ensuring the diagonal consists only of powers of 2. The advantage of this method is the elimination of floating-point errors when computing the balanced matrix on base-two computers, making it ``good enough'' in practice. Some efforts are also made to compare balancing algorithms on real use cases~\cite{schneider1990comparative}.

\paragraph*{Issues in Matrix Balancing} While matrix balancing is generally effective, there are corner cases where it can worsen the conditioning of matrices. \textsc{Watkins} et al. highlighted some of these cases in~\cite{watkins2006case}. Additionally, \textsc{James} et al. explained, in~\cite{james2014matrix}, these issues and proposed modifications to LAPACK to mitigate them.

\paragraph*{Theoretical bounds}  \cite{kalantari1996complexity} establishes an upper bound on the norm of the scaling factors and presents a polynomial-time complexity bound for the computation of scaling factors with a prescribed accuracy.

\paragraph*{Characterizations of Non-Negative Balancable Matrices} \cite{osborne1960pre} and \cite{eaves1985line} offer characterizations of non-negative balancable matrices, contributing to the theoretical understanding of the properties of such matrices. Additionally, \cite{kalantari1997complexity} provides insights into the complexity aspects of this problem.

\paragraph*{Tensor Matrix Balancing} \textsc{Sugiyama} et al. extended matrix balancing to tensors in~\cite{sugiyama2017tensor}, providing a perspective on balancing operations in multi-dimensional spaces. This work contributes to the broader understanding of matrix balancing techniques applied beyond traditional matrices.

\subsection{Contributions of the paper}

Their main contribution is \Cref{main_thm}. It exhibits a variant of \textsc{Osborn}'s algorithm with near-linear runtime in the input sparsity. It also shows that improving the runtime dependence in \(\varepsilon\) can be improve from \(\varepsilon^{-2}\) to \(\varepsilon^{-1}\) without an additional factor \(n\).  

\thm{\label{main_thm}Let \(K \in \M_n(\R_+)\) be a balanceable non negative square matrix and \(\varepsilon \geq 0\). Random \textsc{Osborn} solves \((\varepsilon, 1)\)-approximate matrix balancing problem in \(T\) operations where there exists \(c > 0, \delta > 0\) such that 
\[
    \mathbb E(T) = \mathcal O \left(\dfrac{m}{\varepsilon}\min\left\{\dfrac{1}{\varepsilon}; d\right\} \log \kappa\right)  \quad \text{and} \quad \mathbb P \left(T \leq c \dfrac{m}{\varepsilon}\min\left\{\dfrac{1}{\varepsilon}; d\right\} \log \kappa \log \dfrac{1}{\delta}\right) \geq 1 - \delta
\]
where \(m\) is the number of nonzero entries in \(K\), \(d\) is the diameter of the graph associated to \(K\) and \(\kappa = \sum_{i,j}K_{i,j}/\min_{i,j}{K_{i,j}}\).}

\subsection{Our contributions}

numerics?
limits? 
optimization ?

\section{Main body (10 pages)}

\subsection{Notations}

Certainly, here is the revised subsection:

For convenience and clarity, we consolidate the notations used throughout our discussions. These notations adhere to standard conventions, ensuring consistency and facilitating a concise representation of key elements:

\begin{itemize}
    \item \(n\) denotes the size of the matrix, representing both the number of rows and columns.
    \item \(m\) signifies the count of non-zero coefficients within the matrix.
    \item \(K \in \mathcal M_n(\R_+)\) is the matrix requiring balancing, where elements of \(K\) belong to the set of positive real numbers (\(\R_+\)).
    \item \(\varepsilon\) serves as the parameter governing the accuracy of the balancing process.
    \item \(G_K\) represents the graph associated with \(K\), and \(d\) stands for its diameter.
    \item The conditioning number of \(K\), denoted by \(\kappa\), is computed as the sum of all elements in \(K\) divided by the minimum non-zero element in \(K\).
    \item The \(\exp\) function is applied in an element-wise manner for both vectors and matrices.
\end{itemize}

These standardized notations provide a foundation for our discussions, ensuring a seamless and unambiguous presentation of our analyses and findings.

\subsection{Presentation of the method}


\begin{code}
osborn(|\(K\)|, |\(\varepsilon\)|):
    x = 0 |\(\in \R^n\)|
    while not(is_balanced(|\(\D(e^x) K \D(e^{-x})\)|, |\(\varepsilon\)|)):
        Choose k |\(\in\)| [n] # This is where variants differ
        x += (log(c_k(|\(\D(e^x) K \D(e^{-x})\)|)) - log(r_k(|\(\D(e^x) K \D(e^{-x})\)|)))/2
    return x
\end{code}

There are \textit{many} way to choose the next coordinate to update and hence many variants of the algorithm. The article focuses on four of them:
\begin{itemize}
    \item \textbf{Cyclic \textsc{Osborn}} Cycle through the coordinates. (\textit{eg.} {\color{magenta}1, 2, 3}, {\color{cyan}1, 2, 3}, {\color{red}1,} \dots).
    \item \textbf{Random-Reshuffle Cyclic \textsc{Osborn}} Cycle through the coordinates using a new random permutation for each cycle. (\textit{eg.} {\color{magenta}2, 1, 3}, {\color{cyan}1, 2, 3}, {\color{red}1, 3, 2}, \dots).
    \item \textbf{Greedy \textsc{Osborn}} Choose \(k\) where the imbalance is maximal \textit{eg.}
    \[
        k = \argmax_{k}\left|\sqrt{r_k(\D(e^x) K \D(e^{-x}))} - \sqrt{c_k(\D(e^x) K \D(e^{-x}))}\right|.    
    \]
    \item \textbf{Random \textsc{Osborn}} Uniformly sample \(k\) independently between each call.
\end{itemize}

\subsubsection{Implementation}

The implementations were carried out in Python, providing us with a transparent and customizable framework for thorough experimentation. The source code for our implementations is available on \href{https://github.com/alexicanesse/Near-linear-convergence-of-the-Random-Osborne-algorithm-for-Matrix-Balancing}{GitHub}. The implementation uses sparse representation for matrices and aimed to provide an implementation using data structures as close as possible as those used to compute the theoretical results. However, we were not able to find a detailed presentation of those and had to do our best to match the article. We did not give any focus on bit complexity. The authors recommanded a log-domain implementation to enable a logarithmic bit-complexity for some variants. However, we decided not to follow through in order to increase speed because we did not worry about bit-complexity.\\

At each update of \(x\), we update the balanced matrix by multiplying the corresponding row and column. With the sparse representation, this operation is proportional to \(m\). It is even \(\mathcal O(m/n)\) on average! To compute the optimization function, the sum of elements in \(DAD^{-1}\) is computed in \(\mathcal O (m)\) thanks to the sparse representation and \(||c(A) - r(A)||_1\) is also computed in \(\mathcal O(m)\). Indeed, we use \(||c(A) - r(A)||_1 = \sum_j \left| \sum_i (DAD^{-1} - (DAD^{-1})^\top)_{i,j}\right|\). \textbf{Those techniques are not presented in the article and we had to come up with on our own.}\\

\subsection{Matrix balancing as a convex optimization problem}

A key part of the proofs of convergence given on the paper and other works rely mainly on a seeing the matrix balancing problem as an optimization problem. Let \(\Phi : x \mapsto \log \sum_{i,j} e^{x_i - x_j} K_{i,j}\). The gradient of this function is 
\[
    \nabla \Phi (x) = \dfrac{r\left( \mathcal D(e^x)K\mathcal D(e^{x}) \right) - c\left( \mathcal D(e^x)K\mathcal D(e^{x}) \right)}{\sum_{i,j}\left( \mathcal D(e^x)K\mathcal D(e^{x}) \right)_{i,j}}.    
\]

We can clearly notice a connection with \Cref{norm} and see that \(x\) is a solution to the \(\varepsilon, k\)-matrix balancing problem if and only if \(||\nabla \Phi(x)||_k \leq \varepsilon\). The function \(\Phi\) is convex and approximately solving the convex optimization problem \(\min_x \Phi(x)\) solves the matrix balancing problem.\\

Alexandre d'Aspremont says in his MVA course on convex optimization that writing a problem as a convex optimization problem is almost solving it, which shows how important this statement is.

\subsection{Theoretical guarantees}\label{theoritical}

\begin{lemma}\label{balanceability}
    A matrix \(K \in \mathcal M_n(\R_+^\star)\) is balanceable if and only if its associated graph is strongly connected~\cite{osborne1960pre}.
\end{lemma}

\thm{\label{greedy_thm}Let \(K \in \M_n(\R_+)\) be a balanceable non negative square matrix and \(\varepsilon \geq 0\). Greedy \textsc{Osborn} solves \((\varepsilon, 1)\)-approximate matrix balancing problem in 
\[
    \mathcal O \left(\dfrac{\color{magenta} n^2}{\varepsilon}\min\left\{\dfrac{1}{\varepsilon}; d\right\} \log \kappa\right) 
\]
where \(d\) is the diameter of the graph associated to \(K\) and \(\kappa = \sum_{i,j}K_{i,j}/\min_{i,j}{K_{i,j}}\).}

\thm{\label{main_thm}Let \(K \in \M_n(\R_+)\) be a balanceable non negative square matrix and \(\varepsilon \geq 0\). Random \textsc{Osborn} solves \((\varepsilon, 1)\)-approximate matrix balancing problem in \(T\) operations where there exists \(c > 0, \delta > 0\) such that 
\[
    \mathbb E(T) = \mathcal O \left(\dfrac{\color{magenta} m}{\varepsilon}\min\left\{\dfrac{1}{\varepsilon}; d\right\} \log \kappa\right)  \quad \text{and} \quad \mathbb P \left(T \leq c \dfrac{\color{magenta} m}{\varepsilon}\min\left\{\dfrac{1}{\varepsilon}; d\right\} \log \kappa \log \dfrac{1}{\delta}\right) \geq 1 - \delta
\]
where \(m\) is the number of nonzero entries in \(K\), \(d\) is the diameter of the graph associated to \(K\) and \(\kappa = \sum_{i,j}K_{i,j}/\min_{i,j}{K_{i,j}}\).}

\thm{\label{random_cyclic_thm}Let \(K \in \M_n(\R_+)\) be a balanceable non negative square matrix and \(\varepsilon \geq 0\). Random cyclic \textsc{Osborn} solves \((\varepsilon, 1)\)-approximate matrix balancing problem in \(T\) operations where there exists \(c > 0, \delta > 0\) such that 
\[
    \mathbb E(T) = \mathcal O \left(\dfrac{\color{magenta} mn}{\varepsilon}\min\left\{\dfrac{1}{\varepsilon}; d\right\} \log \kappa\right)  \quad \text{and} \quad \mathbb P \left(T \leq c \dfrac{\color{magenta} mn}{\varepsilon}\min\left\{\dfrac{1}{\varepsilon}; d\right\} \log \kappa \log \dfrac{1}{\delta}\right) \geq 1 - \delta
\]
where \(m\) is the number of nonzero entries in \(K\), \(d\) is the diameter of the graph associated to \(K\) and \(\kappa = \sum_{i,j}K_{i,j}/\min_{i,j}{K_{i,j}}\).}

\subsection{Numerics}

Computing numeric experiments was not easy. Getting the actual number of arithmetic operations is a complex endeavour, especially with a high-level language such as Python. We decided do use the running time to estimate how the number of arithmetic operations will evolve as a function of given parameters. This is not a precise measurement. However, all algorithms presented are are basically the same and this measure should be a fair assessment of convergence speed. 


\subsubsection{Data generation}\label{data_genaration}

In order to conduct numerical experiments, it is useful to be able to generate matrices with given parameters such as value of \(\kappa\) and sparsity. For that, we start by generating a \((n,n)\) random matrix with \(m\) non-zero values using \mintinline{python}{scipy.sparse.random}. Then we assign values to those non-zero entries according to a \textsc{exponential} distribution of scale 1. We then want to modify those values to get the expected value for \(\kappa\). To do so, we add a value \(x\) to all non-zeros entries. If the generated matrix was \(K\), to find \(x\), we solve 
\[
    \kappa = \dfrac{\sum_{i,j} K_{i,j} + m \times x}{K_{\text{min}} + x}.
\]        

We then find that the value to add is 
\[
    x = \dfrac{\kappa \times K_\text{min} - \sum_{i,j}K_{i,j}}{m - \kappa}.   
\]

Sometimes, \(x\) is negative and \(K\) end up not being non-negative and we just start again from the start. Finally, we want the matrix to be balanceable. Hence, we start again until its associated graph is strongly connected (\Cref{balanceability}). 

\subsubsection{Complexity as a function of \(n\)}

In order to experimentally show the convergence speed of theorems presented in~\Cref{theoritical}, we fixed the parameters \(m\) and \(\varepsilon\) and let \(n\) in \(\inte 20 60 \). Each matrix is generated using an exponential distribution according to the scheme\footnote{Note that here \(\kappa\) is not fixed.} presented in~\Cref{data_genaration}. The~\Cref{fig:n} shows the results.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/n/random_function_of_n_n_20_60__kappa_55582.80548329922}
        \caption{Random.}\label{fig:na}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/n/random_cyclic_function_of_n_n_20_60__kappa_300}
        \caption{Random cyclic.}\label{fig:nb}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/n/greedy_function_of_n_n_20_60__kappa_2602746.9752206216}
        \caption{Greedy.}\label{fig:nc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/n/cyclic_function_of_n_n_20_60__kappa_300}
        \caption{Cyclic.}\label{fig:nd}
    \end{subfigure}
    \caption{Plot of \(\text{avg}(\text{time}/(m \min(d, 1/\varepsilon)\log(\kappa)/\varepsilon))\) as a function of \(n\), using various variations with \(\varepsilon = 10^{-4}\) and \(m = 175\). Matrices are generated according to the framework presented in \Cref{data_genaration}. Forall \(n \in \inte 20 60 \), we generate 10 matrices of size \(n\) and sparsity \(m\).}\label{fig:n}
\end{figure}

We can assess the convergence depicted in~\Cref{fig:n} by comparing it to the outcomes presented in~\Cref{theoritical}. As outlined in~\Cref{random_cyclic_thm}, our anticipation was a linear progression concerning \(n\) in~\Cref{fig:nb} and~\Cref{fig:nd}. Remarkably, this linear trend is observed, even though it was originally expected only in terms of expectation. Referring to~\Cref{greedy_thm}, an evolution in \(\mathcal O(n^2/m)\) is expected in~\Cref{fig:nc}, and this prediction aligns with the observed outcomes. These numerical results are indeed promising.\\

The final validation involves examining~\Cref{main_thm}, suggesting that~\Cref{fig:na} should remain constant. Contrary to this expectation, the figure does not exhibit constancy. However, the growth rate is noticeably lower compared to the other variants, and there appears to be a tendency for the curve to plateau as \(n\) increases which is encouraging.


\subsubsection{Complexity as a function of \(m\)}

In order to experimentally show the convergence speed of theorems presented in~\Cref{theoritical}, we fixed the parameters \(n\) and \(\varepsilon\) and let \(m\) in \(\inte 60 300 \). Each matrix is generated using an exponential distribution according to the scheme\footnote{Note that here \(\kappa\) is not fixed.} presented in~\Cref{data_genaration}. The~\Cref{fig:m} displays the results.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/m/random_function_of_m_n_10_m_20_90_kappa_524564.1271422369}
        \caption{Random.}\label{fig:ma}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/m/random_cyclic_function_of_m_n_10_m_20_90_kappa_60194.24212737572}
        \caption{Random cyclic.}\label{fig:mb}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/m/greedy_function_of_m_n_10_m_20_90_kappa_241790.2230925089}
        \caption{Greedy.}\label{fig:mc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/m/cyclic_function_of_m_n_10_m_20_90_kappa_118817.63812510876}
        \caption{Cyclic.}\label{fig:md}
    \end{subfigure}
    \caption{Plot of \(\text{avg}(\text{time}/(\min(d, 1/\varepsilon)\log(\kappa)/\varepsilon))\) as a function of \(m\), using various variations with \(\varepsilon = 10^{-2}\) and \(n = 60\). Matrices are generated according to the framework presented in \Cref{data_genaration}. Forall \(m \in \inte 60 300 \), we generate 10 matrices of size \(n\) and sparsity \(m\).}\label{fig:m}
\end{figure}

Interpreting the plots in~\Cref{fig:m} using the theorems outlined in~\Cref{theoritical} poses challenges due to intrinsic connections between certain variables, such as \(d\) and \(m\) in the results. The expectation, based on~\Cref{greedy_thm}, is that the plot using the greedy \textsc{Osborn} method should remain flat, indicating no dependence on \(m\). However, contrary to this theoretical expectation, the observed plot does not exhibit this characteristic. A more nuanced understanding is provided by~\Cref{fig:m1}, where the relationship between~\Cref{fig:mc} and~\Cref{fig:m1a} suggests non-monotonic behavior.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.475\textwidth}
        \centering
        \includegraphics[width=.75\textwidth]{figures/m/d}
        \caption{Evolution of \(d\).}\label{fig:m1a}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.475\textwidth}
        \centering
        \includegraphics[width=.75\textwidth]{figures/m/kappa}
        \caption{Evolution of \(\log\kappa\).}\label{fig:m1b}
    \end{subfigure}
    \caption{Plots of \(\text{avg}(d)\) and \(\text{avg}(\log\kappa)\) as a function of \(m\), using \(n = 60\) and the framework presented in \Cref{data_genaration}. Forall \(m \in \inte 60 300 \), we generate 10 matrices of size \(n\) and sparsity \(m\).}\label{fig:m1}
\end{figure}

Specifically, as~\Cref{fig:m1a} decreases,~\Cref{fig:mc} increases, and vice versa, elucidating the non-monotonous trend observed. Additionally, insights from~\Cref{fig:m1b} contribute to an understanding of why the scaled time is decreasing overall for the greedy variant. Similar principles apply to the interpretation of the other plots. Notably, the random variant appears to be less affected by overall trends, suggesting a higher dependency on \(m\).\\

Unfortunately, the observed plots do not align with our expectations. They fail to exhibit a clear linear dependency for the three variants, contrary to the anticipated linear relationship.

\subsubsection{Complexity as a function of \(\kappa\)}

Finally, same experiments were ran to showcase the linear dependency in \(\log\kappa\) in theorems outlined in~\Cref{theoritical}. The result, provided in~\Cref{fig:kappa} clearly fails to showcase any dependency in \(\kappa\).

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/kappa/random_function_of_kappa}
        \caption{Random.}\label{fig:kappaa}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/kappa/random_cyclic_function_of_kappa}
        \caption{Random cyclic.}\label{fig:kappab}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/kappa/greedy_function_of_kappa}
        \caption{Greedy.}\label{fig:kappac}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/kappa/cyclic_function_of_kappa}
        \caption{Cyclic.}\label{fig:kappad}
    \end{subfigure}
    \caption{Plot of \(\text{avg}(\text{time}/(\min(d, 1/\varepsilon)/\varepsilon))\) as a function of \(\kappa\), using various variations with \(\varepsilon = 10^{-2}\), \(n = 10\) and \(m = 30\). Matrices are generated according to the framework presented in \Cref{data_genaration}. Forall \(\kappa \in \inte 40 10^6 \), we generate 10 matrices of size \(n\) and sparsity \(m\).}\label{fig:kappa}
\end{figure}

\subsection{Connections between \textsc{Osborn}'s and \textsc{Sinkhorn}'s algorithms}

A connection with \textsc{Sinkhorn}'s Algorithm must be drawn. Those two algorithms compute a scaling of a matrix. The main difference is \textsc{Osborn} returns a scaling of the form \(D A D^{-1}\) whereas \textsc{Sinkhorn} returns a scaling of the form \(XAY\) with \(X\) and \(Y\) diagonals.\\

These two algorithms minimize a distance between two measures. For the first one, it is \textsc{Kullback-Leibler} and for the second one it is \textsc{Hellinger}.\\

\textsc{Sinkhorn} is an optimal transport algorithm, as we have seen in the course. It computes a solution to
\[
    \min_{P \in U(a,b)} \left\langle P, C\right\rangle + \varepsilon \text{KL}(P|\un_{n \times m})  
\]
using notations from the course. Unlike with \textsc{Osborn}'s algorithm, we do not need to work with square matrices.\\

\textsc{Osborn}'s algorithm does not compute an optimal transport. We tried to modify it to be able to use it to compute optimal transport solutions. However, we failed to do so. It still has some advantages : its scaling has a form that preserves eigenspaces.\\

We still wanted to compare those algorithms and the only way we found is to use that \textsc{Sinkhorn}'s algorithm can be used as a balancing algorithm: we just have to use the same distribution as input and output for it. This comparaison is not fair because \textsc{Sinkhorn}'s additional has more constrains as we impose the value of the sum of the rows and columns and also because it is more versatile in general.\\

Those two algorithms are really similar. Hence, we will use the number of iteration to compare them. We are using an implementation of \textsc{Sinkhorn}'s algorithm from numerical tours of the course~\cite{peyre2011numerical} and use the stopping criteria of \textsc{Osborn}'s algorithm:
\[
    \dfrac{||c(A) - r(A)||_k}{\sum_{i,j} a_{i,j}} \leq \varepsilon.
\]
This criterion is justified for \textsc{Sinkhorn}'s algorithm too because we are running it with the same distribution as input and output.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=.3\textwidth]{figures/data_sinkhorn}
    \caption{Data used for our experiments on entropic regularization. The outer set is generated using 30 regularly placed points on a circle of radius one centred at the origine. We added a Gaussian noise on each point with a scale of \(6.10^{-2}\). The inner set is generated by placing regularly 10 points on each edges of an equilateral triangle with edges of length \(\sqrt{3}/3\). We added a Gaussian noise on each point with a scale of \(10^{-2}\).}\label{data}
\end{figure}

In order to generate the matrices on which we will use the algorithms, we decided to use kernels from entropic regularization in optimal transport. We took inspiration from the numerical tours in the course~\cite{peyre2011numerical}. Given a ground cost matrix \(C\), the kernel \(K\) is defined by 
\[
    \forall i,j \quad K_{i,j} = \exp\left( -\dfrac{C_{i,j}}{\varepsilon_K} \right).
\]
We decided to use such kernels because it allows us to test our algorithms on real use cases while being able change parameters such as \(C\) and \(\varepslion_K\) in order to get kernels with various properties.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_0.001/osborn_vs_sinkhorn_dist2_0.001_0.01}
        \caption{\(\varepsilon = 10^{-2}\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_0.001/osborn_vs_sinkhorn_dist2_0.001_0.0001}
        \caption{\(\varepsilon = 10^{-5}\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_0.001/osborn_vs_sinkhorn_dist2_0.001_1e-08}
        \caption{\(\varepsilon = 10^{-8}\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_0.001/osborn_vs_sinkhorn_dist2_0.001_1e-12}
        \caption{\(\varepsilon = 10^{-12}\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_0.001/osborn_vs_sinkhorn_dist2_0.001_1e-16}
        \caption{\(\varepsilon = 10^{-16}\)}
    \end{subfigure}
    \caption{Convergence Analysis of \textsc{Osborn}'s and \textsc{Sinkhorn}'s Algorithms in the Regular Entropic Regularization Setting. The analysis utilizes data from~\Cref{data} with a regularization parameter \(\varepsilon_K\) set to \(10^{-3}\).}\label{fig:firstkernel}
\end{figure}

The~\Cref{fig:firstkernel} shows a first application of the algorithms, including variants of \textsc{Osborn}'s algorithm. We can see that all variants tends to have the same behaviors. This will be confirmed by the following experiments. However, \textsc{Sinkohorn}'s algorithm tends to have a different comportment, it looks more continuous.


\subsubsection{Using different regularization parameters}

The regularization parameter \(\varepsilon_K\) has a high impact on the final result. However, this is not what we are looking for here. We are interested in how its impact on \(K\) impacts the convergence of the algorithms. A smaller value ok \(\varepsilon_K\) means that \(C/\varepsilon_K\) will have larger values. The matrix \(C\) has non-negative values, hence smaller values of the regularization parameter will reduce the gap between values in \(K\).

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_kernel_1e-12/osborn_vs_sinkhorn_dist2_1.0_1e-12}
        \caption{\(\varepsilon_K = 10^{0}\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_kernel_1e-12/osborn_vs_sinkhorn_dist2_0.1_1e-12}
        \caption{\(\varepsilon_K = 10^{-1}\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_kernel_1e-12/osborn_vs_sinkhorn_dist2_0.01_1e-12}
        \caption{\(\varepsilon_K = 10^{-2}\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.23\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_kernel_1e-12/osborn_vs_sinkhorn_dist2_0.001_1e-12}
        \caption{\(\varepsilon_K = 10^{-3}\)}
    \end{subfigure}
    \caption{Convergence Analysis of \textsc{Osborn}'s and \textsc{Sinkhorn}'s Algorithms with Varied Regularization Parameters. The study explores the convergence behavior of both algorithms using different regularization parameters, leveraging data from~\Cref{data}.}\label{fig:regularization}
\end{figure}

The illustration in~\Cref{fig:regularization} provides insights into how the spacing of values in the kernel influences the effectiveness of the algorithms. Notably, when all values in the kernel are closely distributed, \textsc{Sinkhorn}'s algorithm gains an advantage, while a larger gap between values tends to pose challenges for its performance. Variants of \textsc{Osborn}'s algorithm exhibit consistent behavior across multiple runs. As anticipated, the greedy variant showcases superior per-iteration improvement. Interestingly, despite the distinct characteristics and varying performance of these algorithms, they demonstrate a similar asymptotic convergence rate, a finding that brings about a degree of surprise.

\subsubsection{Using different \textsc{Minkowski} distances}\label{minkow}

The regular distance matrix is constructed based on the square of the Euclidean distance, equivalent to the square of the 2-\textsc{Minkowski} distance. To investigate the impact of altering this parameter, we decided to explore how the algorithms respond to variations in this parameter. The cost matrix is then defined, for a parameter \(p \geq 0\), as follows:
\[
    \forall i,j \quad C_{i,j} = \sum_{k} \left|x_{i,k} - y_{i,k}\right|^p.
\]

Intuitively, when \(p\) is in the range \(]0, 1]\), values close to zero become more pronounced, leading to a reduction in the overall scale. On the other hand, larger values of \(p\) cause values in \([0, 1[\) to approach zero, and those higher than 1 to become considerably larger. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/minkowski/osborn_vs_sinkhorn_dist2_0.01_1e-10_0.5_0.5}
        \caption{\(p = 0.5\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/minkowski/osborn_vs_sinkhorn_dist2_0.01_1e-10_1_1}
        \caption{\(p = 1\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/minkowski/osborn_vs_sinkhorn_dist2_0.01_1e-10_5_5}
        \caption{\(p = 5\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/minkowski/osborn_vs_sinkhorn_dist2_0.01_1e-10_10_10}
        \caption{\(p = 10\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/minkowski/osborn_vs_sinkhorn_dist2_0.01_1e-10_50_50}
        \caption{\(p = 50\)}
    \end{subfigure}
    \caption{Convergence Analysis of \textsc{Osborn}'s and \textsc{Sinkhorn}'s Algorithms with Varied \textsc{Minkowski} distances and \(\varepsilon_K = 10^{-2}\). The investigation explores how different parameters of a \textsc{Minkowski} distance impact the convergence behavior of both algorithms, utilizing data from~\Cref{data}.}
\end{figure}

As \(p\) increases, both algorithms exhibit a trend where fewer iterations are needed. All variants of \textsc{Osborn}'s algorithm respond uniformly to this trend and show resilience without apparent failures. However, \textsc{Sinkhorn}'s algorithm encounters challenges, particularly when \(p\) decreases, and it faces convergence issues, being unable to converge for \(p = 0.5\). This observation reinforces the notion that \textsc{Sinkhorn} tends to underperform in scenarios characterized by numerous small values.

\subsubsection{Using different powers of a given distance}

The experiment detailed in~\Cref{minkow} presented intriguing findings. However, it's essential to acknowledge that when \(p\) changes, the matrix's structure itself may undergo substantial alterations due to its impact on the differences between every pair of coordinates. To mitigate this, we opted for a different approach. We selected a specific cost—namely, the squared Euclidean distance—and applied an element-wise power to it. Subsequently, for all parameters \(k\), we defined the cost matrix as follows:
\[
    \forall i,j \quad C_{i,j} = \left(\sum_l (x_{i,l} - y_{j,l})^2\right)^k.
\]

The parameter \(k\) introduces an element-wise scaling, thereby influencing the previously introduced parameter \(\kappa\). This adjustment allows us to investigate the impact of element-wise scaling on the algorithms' behavior, providing additional insights into their sensitivity to different cost structures.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_k/osborn_vs_sinkhorn_dist2_0.01_1e-10_2_0.1}
        \caption{\(k = 0.1\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_k/osborn_vs_sinkhorn_dist2_0.01_1e-10_2_0.5}
        \caption{\(k = 0.5\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_k/osborn_vs_sinkhorn_dist2_0.01_1e-10_2_1}
        \caption{\(k = 1\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_k/osborn_vs_sinkhorn_dist2_0.01_1e-10_2_5}
        \caption{\(k = 5\)}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.18\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/dist2_k/osborn_vs_sinkhorn_dist2_0.01_1e-10_2_10}
        \caption{\(k = 20\)}
    \end{subfigure}
    \caption{Convergence Analysis of \textsc{Osborn}'s and \textsc{Sinkhorn}'s Algorithms with Varied Distance Powers and \(\varepsilon_K = 10^{-2}\). The investigation explores how different powers of a distance impact the convergence behavior of both algorithms, utilizing data from~\Cref{data}.}\label{k}
\end{figure}

The observed results in~\Cref{k} indicate a preference for smaller values of \(k\) by all algorithms, contributing to the compression of costs. This preference aligns with the characteristic that all Euclidean distances in our dataset are greater than one. Notably, \textsc{Sinkhorn}'s algorithm is notably more affected by variations in \(k\), highlighting its sensitivity to changes in the element-wise scaling parameter. 

\subsubsection{Using other distances} 

For our final experiment, we aimed to leverage the insights gathered from previous experiments to construct a cost matrix that would favor \textsc{Osborn} while challenging \textsc{Sinkhorn}, validating our intuitive understanding.\\

Firstly, we constructed a distance matrix using the square Euclidean norm and normalized it by its maximum value. This yielded a matrix with values in the range \(]0,1]\). Subsequently, values below \(0.3\) were set to zero. The thresholded values at 0 were then transformed: those set to 0 were mapped to 1 upon exponentiation, while other non-zero values became extremely small in comparison (\(\leq 1^{-20}\)). This deliberate manipulation resulted in an extremely ill-balanced matrix.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sparse_test/K.pdf}
        \caption{The kernel \(K\).}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sparse_test/conv}
        \caption{Convergence of the criterion.}
    \end{subfigure}
    \caption{Matrix Balancing with \textsc{Osborn} and \textsc{Sinkhorn} Algorithms. The kernel is derived from data in~\Cref{data}. We construct a distance matrix \(D\) by computing the squared Euclidean norm and normalizing it by its maximum value. Subsequently, values below \(0.3\) are thresholded to 0. The kernel \(K\) is then defined as \(K = \exp(-C/\varepsilon_K)\) with \(\varepsilon_K = 10^{-2}\). It is important to note that \(K\) excludes any inputs equal to 0.}\label{ill}
\end{figure}

This experiment yielded successful outcomes, as evident in~\Cref{ill}. \textsc{Osborn} demonstrated convergence with a relatively small number of iterations, whereas \textsc{Sinkhorn} not only struggled but failed to converge entirely. These results provide further support for our earlier intuitions and observations regarding the algorithms' sensitivity to specific cost matrix structures.

\subsubsection{Conclusion}

Exploring various regularization parameters, \textsc{Minkowski} distances, powers, and other distances, we observed distinct behaviors. Notably, \textsc{Sinkhorn}'s sensitivity to parameter variations, especially in ill-balanced matrices, raises questions about its robustness in certain scenarios.

\section{Conclusion and perspective}

We presented matrix balancing and its practical applications, with a specific emphasis on introducing \textsc{Osborn}'s algorithm and its variants as outlined in this article. We provided a comprehensive explanation of the steps required for an efficient implementation, and the clean code is made publicly available on GitHub. However, it's worth noting that the original article lacked a detailed discussion on achieving efficiency, particularly in terms of temporal and spatial complexity. The absence of explicit data structures or supporting resources for their claims was a notable limitation.\\

The theoretical results, pivotal to the article, were presented and complemented with numerical experiments, yielding relative success in showcasing their convergence rate.\\

Moreover, we established a connection between \textsc{Osborn}'s and \textsc{Sinkhorn}'s algorithms, conducting a comparative analysis across various types of kernels for matrix balancing. Our findings illustrated instances where \textsc{Sinkhorn} encounters challenges. Notably, we demonstrated that \textsc{Osborn}'s convergence remains consistent across variants, with a consistent ranking of the number of iterations required in every case. Leveraging these results, we successfully constructed a kernel where \textsc{Osborn} excels while \textsc{Sinkhorn} struggles and fails to converge.

\section{Connexion with the course}

In the context of this paper, several notions, results, and algorithms presented in the course are either used or related, although the connections may sometimes be nuanced due to the unique characteristics of \textsc{Osborn}'s algorithm. The exploration follows a chronological order:

\begin{itemize}
    \item Kantorovich Relaxation:\\
    
    The concept of \textsc{Kantorovich} relaxation is foundational. Admissible couplings are required only to satisfy mass conservation. For two discrete mass distributions \(a\) and \(b\), the set \(U(a,b)\) is defined as:
    \[
    U(a,b) := \left\{P \in \mathcal M_{n,m}(\R_+) \ | P\mathbf{1}_m = a \ \text{and}\ P^\top\mathbf{1}_n = b \right\}.
    \]
    \textsc{Osborn}'s algorithm provides assignments in \(U(a,a)\), although \(a\) is an output of the algorithm, and it may not sum to 1 initially. The connection here emphasizes the role of mass conservation.
    \item Optimal Transport as a Distance:\\
    
    There is a link to the notion that optimal transport can define a distance. While \textsc{Sinkhorn}'s algorithm is practically employed to define a distance~\cite{NIPS2013_af21d0c9, qian2016non}, and given its close relation to \textsc{Osborn}'s algorithm, attempts were made to find analogous definitions. However, these attempts were unsuccessful.
    \item \text{Sinkhorn}'s Algorithm:
    
    The connection to \textsc{Sinkhorn}'s algorithm is evident. Both algorithms perform matrix scaling but with different objectives. The report extensively evaluates the connections between these two algorithms.
    \item Discrete Barycenters:\\
    
    The concept of discrete barycenters is linked to \textsc{Osborn} through a modified problem. Considering the optimization problem involving discrete barycenters, one could run \textsc{Osborn} on a modified problem inspired by the use of \textsc{Sinkhorn} for barycenters. This connection suggests potential applications of \textsc{Osborn} in barycenter-related scenarios.

    \item Unbalanced Optimal Transport:\\
    
    Unbalanced optimal transport is another area of connection. The notion of optimal transport defined without exact mass conservation serves as inspiration for the use of \textsc{Osborn} in the context of optimal transport problems. The application of \textsc{Osborn} in scenarios where mass conservation is not strictly enforced aligns with unbalanced optimal transport formulations.
\end{itemize}

These connections underscore the versatility and adaptability of \textsc{Osborn}'s algorithm, allowing it to find relevance and application in various areas related to optimal transport and related problems discussed in the course.

\newpage
\printbibliography[heading=bibintoc]

\end{document}
